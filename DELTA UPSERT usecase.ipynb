{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b19f4c9-f65d-46b6-b7f2-929036b5dd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------+-----------+--------+-------------------+------------+-----+---------+-------------+-------------------+-----------+\n|customer_id|product_id|transaction_id|sale_amount|discount|          sale_date|product_name|price| category|customer_name|              email|signup_date|\n+-----------+----------+--------------+-----------+--------+-------------------+------------+-----+---------+-------------+-------------------+-----------+\n|          1|       103|             3|      200.0|    20.0|2024-01-03 12:00:00|   Product C|200.0|Category3|        Alice|  alice@example.com| 2023-12-15|\n|          1|       101|             1|       50.0|     5.0|2024-01-01 10:00:00|   Product A| 50.0|Category1|        Alice|  alice@example.com| 2023-12-15|\n|          2|       102|             2|      150.0|    10.0|2024-01-02 11:00:00|   Product B|150.0|Category2|          Bob|    bob@example.com| 2023-11-20|\n|          3|       101|             4|       75.0|     5.0|2024-01-04 13:00:00|   Product A| 50.0|Category1|      Charlie|charlie@example.com| 2023-10-05|\n|          4|       104|             5|      125.0|     7.5|2024-01-05 14:00:00|   Product D|125.0|Category1|        David|  david@example.com| 2023-09-12|\n|          5|       105|             6|      250.0|    12.5|2024-01-06 15:00:00|   Product E|250.0|Category2|          Eve|    eve@example.com| 2023-08-18|\n+-----------+----------+--------------+-----------+--------+-------------------+------------+-----+---------+-------------+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, to_date\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RetailSalesDataPipeline\").getOrCreate()\n",
    "\n",
    "# Sample data for sales_df\n",
    "sales_data = [\n",
    "    (1, 101, 1, 50.0, 5.0, \"2024-01-01 10:00:00\"),\n",
    "    (2, 102, 2, 150.0, 10.0, \"2024-01-02 11:00:00\"),\n",
    "    (3, 103, 1, 200.0, 20.0, \"2024-01-03 12:00:00\"),\n",
    "    (4, 101, 3, 75.0, 5.0, \"2024-01-04 13:00:00\"),\n",
    "    (5, 104, 4, 125.0, 7.5, \"2024-01-05 14:00:00\"),\n",
    "    (6, 105, 5, 250.0, 12.5, \"2024-01-06 15:00:00\")\n",
    "]\n",
    "\n",
    "sales_columns = [\"transaction_id\", \"product_id\", \"customer_id\", \"sale_amount\", \"discount\", \"sale_date\"]\n",
    "sales_df = spark.createDataFrame(sales_data, sales_columns)\n",
    "sales_df = sales_df.withColumn(\"sale_date\", to_timestamp(\"sale_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Sample data for products_df\n",
    "products_data = [\n",
    "    (101, \"Product A\", 50.0, \"Category1\"),\n",
    "    (102, \"Product B\", 150.0, \"Category2\"),\n",
    "    (103, \"Product C\", 200.0, \"Category3\"),\n",
    "    (104, \"Product D\", 125.0, \"Category1\"),\n",
    "    (105, \"Product E\", 250.0, \"Category2\")\n",
    "]\n",
    "\n",
    "products_columns = [\"product_id\", \"product_name\", \"price\", \"category\"]\n",
    "products_df = spark.createDataFrame(products_data, products_columns)\n",
    "\n",
    "# Sample data for customers_df\n",
    "customers_data = [\n",
    "    (1, \"Alice\", \"alice@example.com\", \"2023-12-15\"),\n",
    "    (2, \"Bob\", \"bob@example.com\", \"2023-11-20\"),\n",
    "    (3, \"Charlie\", \"charlie@example.com\", \"2023-10-05\"),\n",
    "    (4, \"David\", \"david@example.com\", \"2023-09-12\"),\n",
    "    (5, \"Eve\", \"eve@example.com\", \"2023-08-18\")\n",
    "]\n",
    "\n",
    "customers_columns = [\"customer_id\", \"customer_name\", \"email\", \"signup_date\"]\n",
    "customers_df = spark.createDataFrame(customers_data, customers_columns)\n",
    "customers_df = customers_df.withColumn(\"signup_date\", to_date(\"signup_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Data Cleaning\n",
    "sales_df = sales_df.na.fill({\"discount\": 0})\n",
    "\n",
    "# Data Enrichment\n",
    "enriched_sales_df = sales_df.join(products_df, on=\"product_id\", how=\"inner\").join(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Save the enriched data to Delta Lake\n",
    "enriched_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/delta/sales_enriched\")\n",
    "\n",
    "# Create a managed Delta table\n",
    "enriched_sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_enriched\")\n",
    "enriched_sales_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4a6019f-4504-4f9d-9cdc-e18124d1936c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------+-----------+--------+-------------------+------------+-----+---------+-------------+-------------------+-----------+\n|customer_id|product_id|transaction_id|sale_amount|discount|          sale_date|product_name|price| category|customer_name|              email|signup_date|\n+-----------+----------+--------------+-----------+--------+-------------------+------------+-----+---------+-------------+-------------------+-----------+\n|          3|       101|             4|       75.0|     5.0|2024-01-04 13:00:00|   Product A| 50.0|Category1|      Charlie|charlie@example.com| 2023-10-05|\n|          4|       104|             5|      125.0|     7.5|2024-01-05 14:00:00|   Product D|125.0|Category1|        David|  david@example.com| 2023-09-12|\n|          1|       101|             1|       60.0|     6.0|2024-01-01 10:00:00|   Product A| 50.0|Category1|        Alice|  alice@example.com| 2023-12-15|\n|          1|       103|             3|      200.0|    20.0|2024-01-03 12:00:00|   Product C|200.0|Category3|        Alice|  alice@example.com| 2023-12-15|\n|          2|       102|             2|      150.0|    10.0|2024-01-02 11:00:00|   Product B|150.0|Category2|          Bob|    bob@example.com| 2023-11-20|\n|          5|       105|             6|      250.0|    12.5|2024-01-06 15:00:00|   Product E|250.0|Category2|          Eve|    eve@example.com| 2023-08-18|\n|          3|       106|             7|      300.0|    15.0|2024-01-07 16:00:00|        null| null|     null|         null|               null|       null|\n+-----------+----------+--------------+-----------+--------+-------------------+------------+-----+---------+-------------+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sample data for updates_df\n",
    "updates_data = [\n",
    "    (1, 101, 1, 60.0, 6.0, \"2024-01-01 10:00:00\"),  # Update existing transaction\n",
    "    (7, 106, 3, 300.0, 15.0, \"2024-01-07 16:00:00\")  # New transaction\n",
    "]\n",
    "\n",
    "updates_columns = [\"transaction_id\", \"product_id\", \"customer_id\", \"sale_amount\", \"discount\", \"sale_date\"]\n",
    "updates_df = spark.createDataFrame(updates_data, updates_columns)\n",
    "updates_df = updates_df.withColumn(\"sale_date\", to_timestamp(\"sale_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Perform the merge (upsert) operation\n",
    "delta_table = DeltaTable.forPath(spark, \"/dbfs/delta/sales_enriched\")\n",
    "\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    updates_df.alias(\"src\"),\n",
    "    \"tgt.product_id = src.product_id AND tgt.sale_date = src.sale_date\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"tgt.sale_amount\": col(\"src.sale_amount\"),\n",
    "    \"tgt.discount\": col(\"src.discount\"),\n",
    "    \"tgt.transaction_id\": col(\"src.transaction_id\"),\n",
    "    \"tgt.customer_id\": col(\"src.customer_id\")\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"product_id\": col(\"src.product_id\"),\n",
    "    \"sale_date\": col(\"src.sale_date\"),\n",
    "    \"sale_amount\": col(\"src.sale_amount\"),\n",
    "    \"discount\": col(\"src.discount\"),\n",
    "    \"transaction_id\": col(\"src.transaction_id\"),\n",
    "    \"customer_id\": col(\"src.customer_id\")\n",
    "}).execute()\n",
    "\n",
    "# Show the updated Delta table\n",
    "updated_df = spark.read.format(\"delta\").load(\"/dbfs/delta/sales_enriched\")\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ee94dd-9c24-4745-acec-3d368f050fd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Explanation of the Merge Operation\n",
    "Load the Delta Table:\n",
    "\n",
    "Load the Delta table you want to perform upserts on.\n",
    "Merge Operation:\n",
    "\n",
    "Use the merge operation to upsert data from the updates_df DataFrame into the Delta table.\n",
    "Match Condition: The condition to match rows from the target and source DataFrames (\"tgt.product_id = src.product_id AND tgt.sale_date = src.sale_date\").\n",
    "When Matched:\n",
    "\n",
    "Update: Specify the columns to update in the target DataFrame with values from the source DataFrame.\n",
    "When Not Matched:\n",
    "\n",
    "Insert: Specify the columns to insert into the target DataFrame with values from the source DataFrame."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DELTA UPSERT usecase",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
