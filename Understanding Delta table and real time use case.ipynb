{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503377fc-aeff-4a08-82ac-261b67435577",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, year, month, dayofmonth\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeEndToEndProject\").getOrCreate()\n",
    "\n",
    "# Drop existing tables and DataFrames if they exist\n",
    "spark.sql(\"DROP TABLE IF EXISTS transactions\")\n",
    "dbutils.fs.rm(\"/dbfs/delta/transactions\", True)\n",
    "\n",
    "# Step 1: Data Ingestion\n",
    "transactions_data = [\n",
    "    (1, 101, 50.0, \"2024-01-01\"),\n",
    "    (2, 102, 150.0, \"2024-01-02\"),\n",
    "    (3, 103, 200.0, \"2024-01-03\"),\n",
    "    (4, 101, 75.0, \"2024-01-04\"),\n",
    "    (5, 104, 125.0, \"2024-01-05\")\n",
    "]\n",
    "transactions_columns = [\"transaction_id\", \"product_id\", \"amount\", \"transaction_date\"]\n",
    "transactions_df = spark.createDataFrame(transactions_data, transactions_columns)\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", to_timestamp(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "delta_table_path = \"/dbfs/delta/transactions\"\n",
    "transactions_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# Step 2: Data Transformation\n",
    "transformed_df = transactions_df.withColumn(\"year\", year(\"transaction_date\")) \\\n",
    "                                .withColumn(\"month\", month(\"transaction_date\")) \\\n",
    "                                .withColumn(\"day\", dayofmonth(\"transaction_date\"))\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ec9f91-33c2-4f63-8d19-157d3fe10b28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Incremental Data Processing\n",
    "transactions_updates_data = [\n",
    "    (2, 102, 175.0, \"2024-01-02\"),\n",
    "    (6, 105, 300.0, \"2024-01-06\")\n",
    "]\n",
    "transactions_updates_columns = [\"transaction_id\", \"product_id\", \"amount\", \"transaction_date\"]\n",
    "transactions_updates_df = spark.createDataFrame(transactions_updates_data, transactions_updates_columns)\n",
    "transactions_updates_df = transactions_updates_df.withColumn(\"transaction_date\", to_timestamp(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "display(transactions_updates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "750d7636-b444-48c8-8dfb-f2ca524b203c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    transactions_updates_df.alias(\"src\"),\n",
    "    \"tgt.transaction_id = src.transaction_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"amount\": \"src.amount\",\n",
    "    \"transaction_date\": \"src.transaction_date\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"transaction_id\": \"src.transaction_id\",\n",
    "    \"product_id\": \"src.product_id\",\n",
    "    \"amount\": \"src.amount\",\n",
    "    \"transaction_date\": \"src.transaction_date\"\n",
    "}).execute()\n",
    "\n",
    "updated_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b51f5f0-4c23-4262-9ba6-ced63400a8bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Data Analysis and Querying\n",
    "updated_df.createOrReplaceTempView(\"transactions_view\")\n",
    "total_sales_per_product = spark.sql(\"\"\"\n",
    "SELECT product_id, SUM(amount) as total_sales\n",
    "FROM transactions_view\n",
    "GROUP BY product_id\n",
    "ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "total_sales_per_product.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02853716-4962-40fb-88cb-24ff3cc69c29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "version_df = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path)\n",
    "version_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2e6632-a386-4851-8e01-054bf349740a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5: Handling ACID Transactions\n",
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "delta_table.restoreToVersion(1)\n",
    "rolled_back_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "rolled_back_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5dc084-453b-49cb-9d36-062b69a0c5c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Optimizing and Managing Data\n",
    "spark.sql(\"OPTIMIZE delta.`{}`\".format(delta_table_path))\n",
    "delta_table.vacuum()\n",
    "spark.sql(\"OPTIMIZE delta.`{}` ZORDER BY (product_id)\".format(delta_table_path))\n",
    "spark.sql(\"COMMENT ON TABLE delta.`{}` IS 'This is a transactions table with sales data.'\".format(delta_table_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a08195b-b254-4ab4-a21b-6b8407ed5992",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Explanation for Step 6 and how it is useful\n",
    "1. Optimizing the Delta Table\n",
    "- Code: spark.sql(\"OPTIMIZE delta.`{}`\".format(delta_table_path))\n",
    "- Explanation: OPTIMIZE Command: This command compacts small files into larger ones. Small files can result from frequent updates, inserts, and deletes, which can degrade query performance.\n",
    "- How It Works: By combining smaller files into larger ones, it reduces the number of files that need to be read during query execution, thus improving performance.\n",
    "\n",
    "2. Vacuuming the Delta Table\n",
    "- Vacuum the Delta table to remove old files\n",
    "- Code: delta_table.vacuum()\n",
    "- Explanation: VACUUM Command: This command removes old data files that are no longer needed for the latest state of the table.\n",
    "- Why It's Needed: Delta Lake keeps old files to allow for time travel and versioning. However, over time, these files can accumulate and consume significant storage space.\n",
    "- How It Works: Vacuuming cleans up these old files based on a retention threshold (default is 7 days).\n",
    "\n",
    "3. Z-Ordering the Delta Table\n",
    "- Z-Ordering for better query performance\n",
    "- Code: spark.sql(\"OPTIMIZE delta.`{}` ZORDER BY (product_id)\".format(delta_table_path))\n",
    "- Explanation: ZORDER Command: This command orders data files by specified columns. Z-ordering is a technique to colocate related information in the same set of files.\n",
    "- Why It's Needed: This improves the performance of read queries that filter on the specified columns (e.g., product_id). By organizing data in this manner, the system can skip large amounts of data that are not relevant to a query, thereby speeding up query execution.\n",
    "\n",
    "4. Adding a Comment to the Table\n",
    "- Add a comment to the table\n",
    "- Code: spark.sql(\"COMMENT ON TABLE transactions IS 'This is a transactions table with sales data.'\")\n",
    "- Explanation: COMMENT Command: Adds a comment to the table metadata. This is useful for documentation and providing context about the table.\n",
    "- Why It's Needed: Helps users understand the purpose of the table and any specific details or notes about the data it contains.\n",
    "\n",
    "Additional Commands\n",
    "Reviewing Table History\n",
    "Display history of transactions\n",
    "history_df = delta_table.history()\n",
    "history_df.show(truncate=False)\n",
    "Explanation:\n",
    "\n",
    "history Method: This method provides the transaction history of the Delta table, showing all the changes made over time.\n",
    "Why It's Useful: It helps in auditing changes, debugging issues, and understanding the evolution of the table's data.\n",
    "Rolling Back to a Previous Version\n",
    "python\n",
    "Copy code\n",
    "- Roll back to a previous version\n",
    "delta_table.restoreToVersion(1)\n",
    "\n",
    "- Show the rolled-back Delta table\n",
    "rolled_back_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "rolled_back_df.show()\n",
    "Explanation:\n",
    "\n",
    "restoreToVersion Method: This method restores the Delta table to a specified previous version.\n",
    "Why It's Needed: Useful for undoing changes that introduced errors or issues, effectively rolling back the state of the table to a known good state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229bd127-19a7-4f76-b7a5-28eecc27d68b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Rolling back to the just previous version\n",
    "- This could be used when a further process failed so you need to drop the whole batch job and roll back to the version collected on the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511cc6b2-f49f-40ff-b274-2a36fe0166bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+-------------------+\n|transaction_id|product_id|amount|   transaction_date|\n+--------------+----------+------+-------------------+\n|             3|       103| 200.0|2024-01-03 00:00:00|\n|             5|       104| 125.0|2024-01-05 00:00:00|\n|             2|       102| 175.0|2024-01-02 00:00:00|\n|             6|       105| 300.0|2024-01-06 00:00:00|\n|             1|       101|  60.0|2024-01-01 00:00:00|\n|             7|       106| 350.0|2024-01-07 00:00:00|\n|             4|       101|  75.0|2024-01-04 00:00:00|\n+--------------+----------+------+-------------------+\n\n+--------------+----------+------+-------------------+\n|transaction_id|product_id|amount|   transaction_date|\n+--------------+----------+------+-------------------+\n|             3|       103| 200.0|2024-01-03 00:00:00|\n|             5|       104| 125.0|2024-01-05 00:00:00|\n|             2|       102| 175.0|2024-01-02 00:00:00|\n|             6|       105| 300.0|2024-01-06 00:00:00|\n|             7|       106| 350.0|2024-01-07 00:00:00|\n|             1|       101|  60.0|2024-02-02 00:00:00|\n|             8|       108| 450.0|2024-01-05 00:00:00|\n|             4|       101|  75.0|2024-01-04 00:00:00|\n+--------------+----------+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeVersioningExample\").getOrCreate()\n",
    "\n",
    "# Drop existing tables and DataFrames if they exist\n",
    "spark.sql(\"DROP TABLE IF EXISTS transactions\")\n",
    "dbutils.fs.rm(\"/dbfs/delta/transactions\", True)\n",
    "\n",
    "# Step 1: Initial Data Ingestion\n",
    "transactions_data = [\n",
    "    (1, 101, 50.0, \"2024-01-01\"),\n",
    "    (2, 102, 150.0, \"2024-01-02\"),\n",
    "    (3, 103, 200.0, \"2024-01-03\"),\n",
    "    (4, 101, 75.0, \"2024-01-04\"),\n",
    "    (5, 104, 125.0, \"2024-01-05\")\n",
    "]\n",
    "transactions_columns = [\"transaction_id\", \"product_id\", \"amount\", \"transaction_date\"]\n",
    "transactions_df = spark.createDataFrame(transactions_data, transactions_columns)\n",
    "transactions_df = transactions_df.withColumn(\"transaction_date\", to_timestamp(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "delta_table_path = \"/dbfs/delta/transactions\"\n",
    "transactions_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# Step 2: Update the Delta Table\n",
    "# First update\n",
    "transactions_updates_data_1 = [\n",
    "    (2, 102, 175.0, \"2024-01-02\"),\n",
    "    (6, 105, 300.0, \"2024-01-06\")\n",
    "]\n",
    "transactions_updates_df_1 = spark.createDataFrame(transactions_updates_data_1, transactions_columns)\n",
    "transactions_updates_df_1 = transactions_updates_df_1.withColumn(\"transaction_date\", to_timestamp(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    transactions_updates_df_1.alias(\"src\"),\n",
    "    \"tgt.transaction_id = src.transaction_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"tgt.amount\": \"src.amount\",\n",
    "    \"tgt.transaction_date\": \"src.transaction_date\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"transaction_id\": \"src.transaction_id\",\n",
    "    \"product_id\": \"src.product_id\",\n",
    "    \"amount\": \"src.amount\",\n",
    "    \"transaction_date\": \"src.transaction_date\"\n",
    "}).execute()\n",
    "\n",
    "# Second update\n",
    "transactions_updates_data_2 = [\n",
    "    (1, 101, 60.0, \"2024-01-01\"),\n",
    "    (7, 106, 350.0, \"2024-01-07\")\n",
    "]\n",
    "transactions_updates_df_2 = spark.createDataFrame(transactions_updates_data_2, transactions_columns)\n",
    "transactions_updates_df_2 = transactions_updates_df_2.withColumn(\"transaction_date\", to_timestamp(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    transactions_updates_df_2.alias(\"src\"),\n",
    "    \"tgt.transaction_id = src.transaction_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"tgt.amount\": \"src.amount\",\n",
    "    \"tgt.transaction_date\": \"src.transaction_date\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"transaction_id\": \"src.transaction_id\",\n",
    "    \"product_id\": \"src.product_id\",\n",
    "    \"amount\": \"src.amount\",\n",
    "    \"transaction_date\": \"src.transaction_date\"\n",
    "}).execute()\n",
    "\n",
    "# Show the updated Delta table\n",
    "updated_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "updated_df.show()\n",
    "\n",
    "# Third update\n",
    "transactions_updates_data_2 = [\n",
    "    (1, 101, 60.0, \"2024-02-02\"),\n",
    "    (8, 108, 450.0, \"2024-01-05\")\n",
    "]\n",
    "transactions_updates_df_2 = spark.createDataFrame(transactions_updates_data_2, transactions_columns)\n",
    "transactions_updates_df_2 = transactions_updates_df_2.withColumn(\"transaction_date\", to_timestamp(\"transaction_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    transactions_updates_df_2.alias(\"src\"),\n",
    "    \"tgt.transaction_id = src.transaction_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"tgt.amount\": \"src.amount\",\n",
    "    \"tgt.transaction_date\": \"src.transaction_date\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"transaction_id\": \"src.transaction_id\",\n",
    "    \"product_id\": \"src.product_id\",\n",
    "    \"amount\": \"src.amount\",\n",
    "    \"transaction_date\": \"src.transaction_date\"\n",
    "}).execute()\n",
    "\n",
    "# Show the updated Delta table\n",
    "updated_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "updated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7dd8d6-5084-4b9d-ac4c-ccdc74e94d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: (3, 2)"
     ]
    }
   ],
   "source": [
    "# Step 3: Fetch the Current Version and Roll Back to Previous Version\n",
    "# Get the latest version of the Delta table\n",
    "latest_version = delta_table.history(1).select(\"version\").collect()[0][0]\n",
    "previous_version = latest_version - 1\n",
    "latest_version, previous_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0fbb45-c9d9-4321-9468-56ac7d183226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "delta_table.history(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23d84192-7d04-48c3-8646-bec035914326",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling back from version 3 to version 2\n+--------------+----------+------+-------------------+\n|transaction_id|product_id|amount|   transaction_date|\n+--------------+----------+------+-------------------+\n|             2|       102| 175.0|2024-01-02 00:00:00|\n|             1|       101|  60.0|2024-01-01 00:00:00|\n|             3|       103| 200.0|2024-01-03 00:00:00|\n|             6|       105| 300.0|2024-01-06 00:00:00|\n|             7|       106| 350.0|2024-01-07 00:00:00|\n|             5|       104| 125.0|2024-01-05 00:00:00|\n|             4|       101|  75.0|2024-01-04 00:00:00|\n+--------------+----------+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rolling back from version {latest_version} to version {previous_version}\")\n",
    "\n",
    "# Restore to the previous version\n",
    "delta_table.restoreToVersion(previous_version)\n",
    "\n",
    "# Show the rolled-back Delta table\n",
    "rolled_back_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "rolled_back_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a726bfb-0de4-4770-86ea-1bb2d0f0230c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+-------------------+\n|transaction_id|product_id|amount|   transaction_date|\n+--------------+----------+------+-------------------+\n|             1|       101|  50.0|2024-01-01 00:00:00|\n|             2|       102| 175.0|2024-01-02 00:00:00|\n|             3|       103| 200.0|2024-01-03 00:00:00|\n|             6|       105| 300.0|2024-01-06 00:00:00|\n|             5|       104| 125.0|2024-01-05 00:00:00|\n|             4|       101|  75.0|2024-01-04 00:00:00|\n+--------------+----------+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Restore to the previous version\n",
    "delta_table.restoreToVersion(1)\n",
    "\n",
    "# Show the rolled-back Delta table\n",
    "rolled_back_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "rolled_back_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92f76da-acfb-4680-8af8-aea227ebf0da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+-------------------+\n|transaction_id|product_id|amount|   transaction_date|\n+--------------+----------+------+-------------------+\n|             2|       102| 175.0|2024-01-02 00:00:00|\n|             1|       101|  60.0|2024-02-02 00:00:00|\n|             3|       103| 200.0|2024-01-03 00:00:00|\n|             6|       105| 300.0|2024-01-06 00:00:00|\n|             7|       106| 350.0|2024-01-07 00:00:00|\n|             8|       108| 450.0|2024-01-05 00:00:00|\n|             5|       104| 125.0|2024-01-05 00:00:00|\n|             4|       101|  75.0|2024-01-04 00:00:00|\n+--------------+----------+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Can we restore the latest version again? Yes\n",
    "delta_table.restoreToVersion(3)\n",
    "\n",
    "# Show the rolled-back Delta table\n",
    "rolled_back_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "rolled_back_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Understanding Delta table and real time use case",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
